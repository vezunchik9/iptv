#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üöÄ –ê–°–ò–ù–•–†–û–ù–ù–´–ô –ü–†–û–í–ï–†–©–ò–ö –ü–û–¢–û–ö–û–í
–ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç—ã—Å—è—á –∫–∞–Ω–∞–ª–æ–≤ —Å –≥–∞—Ä–∞–Ω—Ç–∏–µ–π –∫–∞—á–µ—Å—Ç–≤–∞ 95%+

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ 50+ –ø–æ—Ç–æ–∫–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
- –£–º–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (6 —á–∞—Å–æ–≤)
- –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏ –¥–ª—è –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
- –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ uptime
- √ó10-15 –±—ã—Å—Ç—Ä–µ–µ –æ–±—ã—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
"""

import asyncio
import aiohttp
import sqlite3
import time
import json
import sys
from datetime import datetime, timedelta
from pathlib import Path
from urllib.parse import urlparse
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class StreamCache:
    """–ö—ç—à —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ—Ç–æ–∫–æ–≤"""
    
    def __init__(self, db_path='stream_cache.db'):
        self.db_path = Path(__file__).parent.parent / db_path
        self.conn = sqlite3.connect(str(self.db_path))
        self.create_tables()
    
    def create_tables(self):
        """–°–æ–∑–¥–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è –∫—ç—à–∞"""
        self.conn.execute('''
            CREATE TABLE IF NOT EXISTS stream_checks (
                url TEXT PRIMARY KEY,
                is_working INTEGER,
                checked_at TIMESTAMP,
                response_time REAL,
                attempts INTEGER DEFAULT 1,
                success_rate REAL DEFAULT 100.0,
                last_error TEXT
            )
        ''')
        self.conn.execute('''
            CREATE INDEX IF NOT EXISTS idx_checked_at 
            ON stream_checks(checked_at)
        ''')
        self.conn.commit()
    
    def get_cached(self, url, max_age_hours=6):
        """–ü–æ–ª—É—á–∏—Ç—å –∏–∑ –∫—ç—à–∞ –µ—Å–ª–∏ –º–æ–ª–æ–∂–µ N —á–∞—Å–æ–≤"""
        cursor = self.conn.execute('''
            SELECT is_working, response_time, success_rate 
            FROM stream_checks 
            WHERE url = ? AND checked_at > ?
        ''', (url, datetime.now() - timedelta(hours=max_age_hours)))
        
        result = cursor.fetchone()
        if result:
            return {
                'is_working': bool(result[0]),
                'response_time': result[1],
                'success_rate': result[2]
            }
        return None
    
    def set_cached(self, url, is_working, response_time, error=None):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏"""
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        cursor = self.conn.execute('''
            SELECT attempts, success_rate FROM stream_checks WHERE url = ?
        ''', (url,))
        prev = cursor.fetchone()
        
        if prev:
            attempts = prev[0] + 1
            prev_rate = prev[1]
            # –û–±–Ω–æ–≤–ª—è–µ–º success rate
            success_rate = (prev_rate * (attempts - 1) + (100 if is_working else 0)) / attempts
        else:
            attempts = 1
            success_rate = 100.0 if is_working else 0.0
        
        self.conn.execute('''
            INSERT OR REPLACE INTO stream_checks 
            (url, is_working, checked_at, response_time, attempts, success_rate, last_error)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (url, is_working, datetime.now(), response_time, attempts, success_rate, error))
        self.conn.commit()
    
    def get_statistics(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—Å–µ–º –ø–æ—Ç–æ–∫–∞–º"""
        cursor = self.conn.execute('''
            SELECT 
                COUNT(*) as total,
                SUM(CASE WHEN is_working = 1 THEN 1 ELSE 0 END) as working,
                AVG(response_time) as avg_response,
                AVG(success_rate) as avg_success_rate
            FROM stream_checks
            WHERE checked_at > ?
        ''', (datetime.now() - timedelta(hours=24),))
        
        result = cursor.fetchone()
        return {
            'total': result[0] or 0,
            'working': result[1] or 0,
            'avg_response_time': result[2] or 0,
            'avg_success_rate': result[3] or 0
        }
    
    def cleanup_old(self, days=7):
        """–£–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏"""
        self.conn.execute('''
            DELETE FROM stream_checks 
            WHERE checked_at < ?
        ''', (datetime.now() - timedelta(days=days),))
        self.conn.commit()
    
    def close(self):
        self.conn.close()


class AsyncStreamChecker:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø—Ä–æ–≤–µ—Ä—â–∏–∫ –ø–æ—Ç–æ–∫–æ–≤"""
    
    def __init__(self, 
                 max_concurrent=50,
                 timeout=5,
                 retry_attempts=2,
                 cache_hours=6):
        """
        Args:
            max_concurrent: –ú–∞–∫—Å–∏–º—É–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫
            timeout: –¢–∞–π–º–∞—É—Ç –Ω–∞ –∫–∞–∂–¥—É—é –ø—Ä–æ–≤–µ—Ä–∫—É (—Å–µ–∫)
            retry_attempts: –ü–æ–ø—ã—Ç–æ–∫ –¥–ª—è –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
            cache_hours: –í—Ä–µ–º—è –∂–∏–∑–Ω–∏ –∫—ç—à–∞ (—á–∞—Å–æ–≤)
        """
        self.max_concurrent = max_concurrent
        self.timeout = timeout
        self.retry_attempts = retry_attempts
        self.cache_hours = cache_hours
        self.cache = StreamCache()
        
        # –°—á–µ—Ç—á–∏–∫–∏
        self.stats = {
            'total': 0,
            'checked': 0,
            'cached': 0,
            'working': 0,
            'failed': 0,
            'start_time': time.time()
        }
    
    async def check_stream(self, session, url):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–¥–∏–Ω –ø–æ—Ç–æ–∫"""
        start_time = time.time()
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
            cached = self.cache.get_cached(url, self.cache_hours)
            if cached:
                self.stats['cached'] += 1
                return {
                    'url': url,
                    'is_working': cached['is_working'],
                    'cached': True,
                    'response_time': cached['response_time'],
                    'success_rate': cached['success_rate']
                }
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ç–æ–∫
            for attempt in range(self.retry_attempts):
                try:
                    async with session.head(
                        url, 
                        timeout=aiohttp.ClientTimeout(total=self.timeout),
                        allow_redirects=True
                    ) as response:
                        response_time = time.time() - start_time
                        is_working = response.status in [200, 301, 302]
                        
                        if is_working:
                            self.cache.set_cached(url, True, response_time)
                            self.stats['working'] += 1
                            return {
                                'url': url,
                                'is_working': True,
                                'cached': False,
                                'response_time': response_time,
                                'status_code': response.status
                            }
                        
                        # –ï—Å–ª–∏ 403/404/500 - –ø—Ä–æ–±—É–µ–º GET –∑–∞–ø—Ä–æ—Å
                        if response.status in [403, 404] and attempt == 0:
                            continue
                            
                except asyncio.TimeoutError:
                    if attempt < self.retry_attempts - 1:
                        await asyncio.sleep(0.5)
                        continue
                except Exception as e:
                    if attempt < self.retry_attempts - 1:
                        await asyncio.sleep(0.5)
                        continue
            
            # –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ—É–¥–∞—á–Ω—ã
            response_time = time.time() - start_time
            self.cache.set_cached(url, False, response_time, "Failed after retries")
            self.stats['failed'] += 1
            
            return {
                'url': url,
                'is_working': False,
                'cached': False,
                'response_time': response_time
            }
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ {url}: {e}")
            self.cache.set_cached(url, False, time.time() - start_time, str(e))
            self.stats['failed'] += 1
            return {
                'url': url,
                'is_working': False,
                'error': str(e)
            }
        finally:
            self.stats['checked'] += 1
    
    async def check_batch(self, urls):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–∞–∫–µ—Ç URL –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ"""
        connector = aiohttp.TCPConnector(limit=self.max_concurrent)
        timeout = aiohttp.ClientTimeout(total=self.timeout)
        
        async with aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
        ) as session:
            tasks = [self.check_stream(session, url) for url in urls]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            return results
    
    def check_m3u_file(self, m3u_file):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤—Å–µ –ø–æ—Ç–æ–∫–∏ –≤ M3U —Ñ–∞–π–ª–µ"""
        logger.info(f"üì∫ –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª: {m3u_file}")
        
        # –ß–∏—Ç–∞–µ–º M3U —Ñ–∞–π–ª
        with open(m3u_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º URL –∏ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–∞–Ω–∞–ª–æ–≤
        lines = content.split('\n')
        channels = []
        current_name = None
        
        for line in lines:
            line = line.strip()
            if line.startswith('#EXTINF'):
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞
                parts = line.split(',', 1)
                if len(parts) > 1:
                    current_name = parts[1].strip()
            elif line and not line.startswith('#'):
                # –≠—Ç–æ URL
                if current_name:
                    channels.append({
                        'name': current_name,
                        'url': line
                    })
                current_name = None
        
        self.stats['total'] = len(channels)
        logger.info(f"  –ù–∞–π–¥–µ–Ω–æ –∫–∞–Ω–∞–ª–æ–≤: {len(channels)}")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ URL –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        urls = [ch['url'] for ch in channels]
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        results = loop.run_until_complete(self.check_batch(urls))
        loop.close()
        
        # –°–æ–∑–¥–∞–µ–º –∫–∞—Ä—Ç—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        url_to_result = {r['url']: r for r in results if isinstance(r, dict)}
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫ –∫–∞–Ω–∞–ª–∞–º
        working_channels = []
        for channel in channels:
            result = url_to_result.get(channel['url'], {})
            if result.get('is_working', False):
                channel['check_result'] = result
                working_channels.append(channel)
        
        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        elapsed = time.time() - self.stats['start_time']
        logger.info(f"\n{'='*60}")
        logger.info(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–†–û–í–ï–†–ö–ò")
        logger.info(f"{'='*60}")
        logger.info(f"–í—Å–µ–≥–æ –∫–∞–Ω–∞–ª–æ–≤:      {self.stats['total']}")
        logger.info(f"–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ:          {self.stats['checked']}")
        logger.info(f"–ò–∑ –∫—ç—à–∞:            {self.stats['cached']}")
        logger.info(f"‚úÖ –†–∞–±–æ—Ç–∞—é—Ç:        {self.stats['working']} ({self.stats['working']/self.stats['total']*100:.1f}%)")
        logger.info(f"‚ùå –ù–µ —Ä–∞–±–æ—Ç–∞—é—Ç:     {self.stats['failed']} ({self.stats['failed']/self.stats['total']*100:.1f}%)")
        logger.info(f"‚è±Ô∏è  –í—Ä–µ–º—è:           {elapsed:.1f} —Å–µ–∫ ({self.stats['total']/elapsed:.1f} –∫–∞–Ω–∞–ª–æ–≤/—Å–µ–∫)")
        logger.info(f"{'='*60}\n")
        
        return working_channels, channels
    
    def save_working_channels(self, working_channels, original_file):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–æ–ª—å–∫–æ —Ä–∞–±–æ—á–∏–µ –∫–∞–Ω–∞–ª—ã"""
        output_file = original_file
        backup_file = f"{original_file}.backup.{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # –ë—ç–∫–∞–ø –æ—Ä–∏–≥–∏–Ω–∞–ª–∞
        import shutil
        shutil.copy2(original_file, backup_file)
        logger.info(f"üíæ –ë—ç–∫–∞–ø: {backup_file}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ —Ä–∞–±–æ—á–∏–µ
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write('#EXTM3U\n')
            for channel in working_channels:
                f.write(f"#EXTINF:-1,{channel['name']}\n")
                f.write(f"{channel['url']}\n")
        
        logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ —Ä–∞–±–æ—á–∏—Ö –∫–∞–Ω–∞–ª–æ–≤: {len(working_channels)}")
        
        return output_file


def main():
    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python3 async_stream_checker.py <file.m3u>")
        sys.exit(1)
    
    m3u_file = Path(sys.argv[1])
    if not m3u_file.exists():
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {m3u_file}")
        sys.exit(1)
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–≤–µ—Ä—â–∏–∫
    checker = AsyncStreamChecker(
        max_concurrent=50,  # 50 –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫
        timeout=5,          # 5 —Å–µ–∫ —Ç–∞–π–º–∞—É—Ç
        retry_attempts=2,   # 2 –ø–æ–ø—ã—Ç–∫–∏
        cache_hours=6       # –ö—ç—à –Ω–∞ 6 —á–∞—Å–æ–≤
    )
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª
    working, all_channels = checker.check_m3u_file(m3u_file)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ —Ä–∞–±–æ—á–∏–µ
    if working:
        checker.save_working_channels(working, m3u_file)
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫—ç—à–∞
    cache_stats = checker.cache.get_statistics()
    logger.info(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞ (24 —á–∞—Å–∞):")
    logger.info(f"  –ó–∞–ø–∏—Å–µ–π: {cache_stats['total']}")
    logger.info(f"  –†–∞–±–æ—Ç–∞—é—Ç: {cache_stats['working']}")
    logger.info(f"  –°—Ä–µ–¥–Ω–∏–π uptime: {cache_stats['avg_success_rate']:.1f}%")
    logger.info(f"  –°—Ä–µ–¥–Ω–∏–π –æ—Ç–∫–ª–∏–∫: {cache_stats['avg_response_time']:.2f} —Å–µ–∫")
    
    # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π
    checker.cache.cleanup_old(days=7)
    checker.cache.close()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
    success_rate = (len(working) / len(all_channels) * 100) if all_channels else 0
    
    if success_rate >= 95:
        logger.info(f"\nüéâ –û–¢–õ–ò–ß–ù–û! Success rate: {success_rate:.1f}% (—Ü–µ–ª—å: 95%)")
    elif success_rate >= 90:
        logger.info(f"\n‚úÖ –•–û–†–û–®–û! Success rate: {success_rate:.1f}% (–ø–æ—á—Ç–∏ —Ü–µ–ª—å: 95%)")
    else:
        logger.info(f"\n‚ö†Ô∏è  Success rate: {success_rate:.1f}% (—Ü–µ–ª—å: 95%, —Ç—Ä–µ–±—É–µ—Ç—Å—è —É–ª—É—á—à–µ–Ω–∏–µ)")

if __name__ == "__main__":
    main()

