#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–£–º–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –ø–ª–µ–π–ª–∏—Å—Ç–æ–≤ —Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å—Å—ã–ª–æ–∫
–ú–æ–∂–µ—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å URL –∫–∞–Ω–∞–ª–æ–≤ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –∏–ª–∏ –¥–æ–±–∞–≤–ª—è—Ç—å –Ω–æ–≤—ã–µ
"""

import requests
import re
import json
import os
from datetime import datetime
from urllib.parse import urlparse
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SmartPlaylistParser:
    def __init__(self, config_file='donors_config.json'):
        self.config = self.load_config(config_file)
        self.stats = {
            'total_parsed': 0,
            'added_channels': 0,
            'updated_channels': 0,
            'skipped_channels': 0,
            'categories_updated': set()
        }
    
    def load_config(self, config_file):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
            if 'update_settings' not in config:
                config['update_settings'] = {
                    'update_existing_urls': True,
                    'match_by_name': True,
                    'match_similarity_threshold': 0.8,
                    'backup_before_update': True
                }
            
            logger.info(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {config_file}")
            return config
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            return {
                "donors": {},
                "category_mapping": {},
                "settings": {"auto_update": True},
                "update_settings": {
                    "update_existing_urls": True,
                    "match_by_name": True,
                    "match_similarity_threshold": 0.8,
                    "backup_before_update": True
                }
            }
    
    def download_playlist(self, url, timeout=30):
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –ø–ª–µ–π–ª–∏—Å—Ç —Å URL"""
        try:
            logger.info(f"–°–∫–∞—á–∏–≤–∞–µ–º –ø–ª–µ–π–ª–∏—Å—Ç: {url}")
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=timeout)
            response.raise_for_status()
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É
            content = response.content
            encodings = ['utf-8', 'cp1251', 'latin1']
            
            for encoding in encodings:
                try:
                    playlist_content = content.decode(encoding)
                    logger.info(f"–ü–ª–µ–π–ª–∏—Å—Ç –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π {encoding}, —Ä–∞–∑–º–µ—Ä: {len(playlist_content)} —Å–∏–º–≤–æ–ª–æ–≤")
                    return playlist_content
                except UnicodeDecodeError:
                    continue
            
            # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ
            playlist_content = content.decode('utf-8', errors='ignore')
            logger.warning("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∫–∞ —Å –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ–º –æ—à–∏–±–æ–∫")
            return playlist_content
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ –ø–ª–µ–π–ª–∏—Å—Ç–∞ {url}: {e}")
            return None
    
    def parse_m3u_playlist(self, content):
        """–ü–∞—Ä—Å–∏—Ç M3U –ø–ª–µ–π–ª–∏—Å—Ç –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∫–∞–Ω–∞–ª—ã"""
        channels = []
        lines = content.splitlines()
        current_extinf = None
        
        for line in lines:
            line = line.strip()
            
            if line.startswith('#EXTINF'):
                current_extinf = line
            elif line and not line.startswith('#') and current_extinf:
                # –ü–∞—Ä—Å–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞–Ω–∞–ª–µ
                channel_info = self.parse_extinf(current_extinf)
                channel_info['url'] = line
                channels.append(channel_info)
                current_extinf = None
        
        logger.info(f"–†–∞—Å–ø–∞—Ä—Å–µ–Ω–æ –∫–∞–Ω–∞–ª–æ–≤: {len(channels)}")
        return channels
    
    def parse_extinf(self, extinf_line):
        """–ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–æ–∫—É #EXTINF –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ"""
        info = {
            'name': '',
            'group': '',
            'logo': '',
            'tvg_id': '',
            'original_extinf': extinf_line
        }
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞ (–ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π –∑–∞–ø—è—Ç–æ–π)
        name_match = re.search(r',([^,]+)$', extinf_line)
        if name_match:
            info['name'] = name_match.group(1).strip()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã
        attributes = {
            'group': r'group-title="([^"]*)"',
            'logo': r'tvg-logo="([^"]*)"',
            'tvg_id': r'tvg-id="([^"]*)"'
        }
        
        for attr, pattern in attributes.items():
            match = re.search(pattern, extinf_line)
            if match:
                info[attr] = match.group(1).strip()
        
        return info
    
    def categorize_channel(self, channel):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∫–∞–Ω–∞–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        name = channel['name'].lower()
        group = channel['group'].lower()
        text_to_check = f"{name} {group}"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        for category, rules in self.config.get('category_mapping', {}).items():
            keywords = rules.get('keywords', [])
            exclude = rules.get('exclude', [])
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            keyword_found = False
            for keyword in keywords:
                if keyword.lower() in text_to_check:
                    keyword_found = True
                    break
            
            if keyword_found:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è
                excluded = False
                for exclude_word in exclude:
                    if exclude_word.lower() in text_to_check:
                        excluded = True
                        break
                
                if not excluded:
                    return category
        
        return None
    
    def normalize_channel_name(self, name):
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–∞–Ω–∞–ª–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        normalized = re.sub(r'[^\w\s]', '', name.lower())
        normalized = re.sub(r'\s+', ' ', normalized).strip()
        return normalized
    
    def calculate_similarity(self, name1, name2):
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–π –∫–∞–Ω–∞–ª–æ–≤ (–ø—Ä–æ—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è)"""
        norm1 = self.normalize_channel_name(name1)
        norm2 = self.normalize_channel_name(name2)
        
        if norm1 == norm2:
            return 1.0
        
        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ
        if norm1 in norm2 or norm2 in norm1:
            return 0.8
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Å–ª–æ–≤–∞–º
        words1 = set(norm1.split())
        words2 = set(norm2.split())
        
        if words1 and words2:
            intersection = len(words1.intersection(words2))
            union = len(words1.union(words2))
            return intersection / union if union > 0 else 0.0
        
        return 0.0
    
    def load_existing_channels(self, category_file):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–∞–Ω–∞–ª—ã –∏–∑ —Ñ–∞–π–ª–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –∏—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        if not os.path.exists(category_file):
            return {}
        
        existing_channels = {}  # {url: {'name': name, 'extinf': extinf_line}}
        
        try:
            with open(category_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            current_extinf = None
            for line in lines:
                line = line.strip()
                
                if line.startswith('#EXTINF'):
                    current_extinf = line
                elif line and not line.startswith('#') and current_extinf:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–∑ EXTINF
                    name_match = re.search(r',([^,]+)$', current_extinf)
                    name = name_match.group(1).strip() if name_match else 'Unknown'
                    
                    existing_channels[line] = {
                        'name': name,
                        'extinf': current_extinf
                    }
                    current_extinf = None
                    
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {category_file}: {e}")
        
        return existing_channels
    
    def find_matching_channel(self, new_channel, existing_channels):
        """–ò—â–µ—Ç —Å–æ–≤–ø–∞–¥–∞—é—â–∏–π –∫–∞–Ω–∞–ª –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é"""
        if not self.config.get('update_settings', {}).get('match_by_name', True):
            return None
        
        threshold = self.config.get('update_settings', {}).get('match_similarity_threshold', 0.8)
        new_name = new_channel['name']
        
        best_match = None
        best_similarity = 0.0
        
        for url, channel_info in existing_channels.items():
            existing_name = channel_info['name']
            similarity = self.calculate_similarity(new_name, existing_name)
            
            if similarity >= threshold and similarity > best_similarity:
                best_similarity = similarity
                best_match = url
        
        return best_match
    
    def backup_category_file(self, category_file):
        """–°–æ–∑–¥–∞–µ—Ç –±—ç–∫–∞–ø —Ñ–∞–π–ª–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        if not os.path.exists(category_file):
            return
        
        backup_dir = "backups/categories"
        os.makedirs(backup_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        category_name = os.path.basename(category_file)
        backup_file = f"{backup_dir}/{category_name}.backup.{timestamp}"
        
        try:
            with open(category_file, 'r', encoding='utf-8') as src:
                with open(backup_file, 'w', encoding='utf-8') as dst:
                    dst.write(src.read())
            logger.info(f"–°–æ–∑–¥–∞–Ω –±—ç–∫–∞–ø: {backup_file}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –±—ç–∫–∞–ø–∞: {e}")
    
    def update_channel_in_category(self, old_url, new_channel, category_file):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç URL –∫–∞–Ω–∞–ª–∞ –≤ —Ñ–∞–π–ª–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        try:
            # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
            with open(category_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # –ò—â–µ–º –∏ –∑–∞–º–µ–Ω—è–µ–º URL
            updated_lines = []
            i = 0
            while i < len(lines):
                line = lines[i].strip()
                
                if line == old_url:
                    # –ó–∞–º–µ–Ω—è–µ–º EXTINF —Å—Ç—Ä–æ–∫—É (–ø—Ä–µ–¥—ã–¥—É—â–∞—è —Å—Ç—Ä–æ–∫–∞)
                    if i > 0 and updated_lines[-1].strip().startswith('#EXTINF'):
                        updated_lines[-1] = new_channel['original_extinf'] + '\n'
                    # –ó–∞–º–µ–Ω—è–µ–º URL
                    updated_lines.append(new_channel['url'] + '\n')
                    logger.info(f"–û–±–Ω–æ–≤–ª–µ–Ω URL –¥–ª—è –∫–∞–Ω–∞–ª–∞ '{new_channel['name']}': {old_url} -> {new_channel['url']}")
                else:
                    updated_lines.append(lines[i])
                
                i += 1
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            with open(category_file, 'w', encoding='utf-8') as f:
                f.writelines(updated_lines)
            
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∫–∞–Ω–∞–ª–∞ –≤ {category_file}: {e}")
            return False
    
    def is_channel_filtered(self, channel):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –∏—Å–∫–ª—é—á–∏—Ç—å –∫–∞–Ω–∞–ª –ø–æ –≥–ª–æ–±–∞–ª—å–Ω—ã–º —Ñ–∏–ª—å—Ç—Ä–∞–º"""
        filters = self.config.get('global_filters', {})
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –∫–∞–Ω–∞–ª–∞
        exclude_channels = filters.get('exclude_channels', [])
        for exclude_pattern in exclude_channels:
            if exclude_pattern.lower() in channel['name'].lower():
                logger.info(f"–ö–∞–Ω–∞–ª '{channel['name']}' –∏—Å–∫–ª—é—á–µ–Ω –ø–æ —Ñ–∏–ª—å—Ç—Ä—É –Ω–∞–∑–≤–∞–Ω–∏—è: {exclude_pattern}")
                return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è –ø–æ URL
        exclude_urls = filters.get('exclude_urls', [])
        for exclude_pattern in exclude_urls:
            if exclude_pattern in channel['url']:
                logger.info(f"–ö–∞–Ω–∞–ª '{channel['name']}' –∏—Å–∫–ª—é—á–µ–Ω –ø–æ —Ñ–∏–ª—å—Ç—Ä—É URL: {exclude_pattern}")
                return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É URL
        min_url_length = filters.get('min_url_length', 10)
        if len(channel['url']) < min_url_length:
            logger.info(f"–ö–∞–Ω–∞–ª '{channel['name']}' –∏—Å–∫–ª—é—á–µ–Ω: URL —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π")
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
        exclude_patterns = filters.get('exclude_patterns', [])
        for pattern in exclude_patterns:
            if re.search(pattern, channel['name'], re.IGNORECASE):
                logger.info(f"–ö–∞–Ω–∞–ª '{channel['name']}' –∏—Å–∫–ª—é—á–µ–Ω –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É: {pattern}")
                return True
        
        return False

    def add_or_update_channel(self, channel, category):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–π –∫–∞–Ω–∞–ª –∏–ª–∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã
        if self.is_channel_filtered(channel):
            self.stats['skipped_channels'] += 1
            return False
            
        category_file = f"categories/{category}.m3u"
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É categories –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        os.makedirs("categories", exist_ok=True)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–∞–Ω–∞–ª—ã
        existing_channels = self.load_existing_channels(category_file)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ URL
        if channel['url'] in existing_channels:
            logger.debug(f"–ö–∞–Ω–∞–ª —Å —Ç–∞–∫–∏–º URL —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {channel['name']}")
            self.stats['skipped_channels'] += 1
            return False
        
        # –ò—â–µ–º –∫–∞–Ω–∞–ª —Å –ø–æ—Ö–æ–∂–∏–º –Ω–∞–∑–≤–∞–Ω–∏–µ–º –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
        if self.config.get('update_settings', {}).get('update_existing_urls', True):
            matching_url = self.find_matching_channel(channel, existing_channels)
            
            if matching_url:
                # –°–æ–∑–¥–∞–µ–º –±—ç–∫–∞–ø –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
                if self.config.get('update_settings', {}).get('backup_before_update', True):
                    self.backup_category_file(category_file)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–∞–Ω–∞–ª
                if self.update_channel_in_category(matching_url, channel, category_file):
                    self.stats['updated_channels'] += 1
                    self.stats['categories_updated'].add(category)
                    return True
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞–∫ –Ω–æ–≤—ã–π –∫–∞–Ω–∞–ª
        try:
            # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            if not os.path.exists(category_file):
                with open(category_file, 'w', encoding='utf-8') as f:
                    f.write("#EXTM3U\n")
                    f.write(f"# –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}\n")
                    f.write("# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–æ\n\n")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞–Ω–∞–ª –≤ –∫–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞
            with open(category_file, 'a', encoding='utf-8') as f:
                f.write(f"{channel['original_extinf']}\n")
                f.write(f"{channel['url']}\n\n")
            
            logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω –Ω–æ–≤—ã–π –∫–∞–Ω–∞–ª '{channel['name']}' –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏—é '{category}'")
            self.stats['added_channels'] += 1
            self.stats['categories_updated'].add(category)
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –∫–∞–Ω–∞–ª–∞ –≤ {category_file}: {e}")
            return False
    
    def process_donor_playlist(self, donor_name, donor_config):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –¥–æ–Ω–æ—Ä—Å–∫–∏–π –ø–ª–µ–π–ª–∏—Å—Ç"""
        if not donor_config.get('enabled', True):
            logger.info(f"–î–æ–Ω–æ—Ä {donor_name} –æ—Ç–∫–ª—é—á–µ–Ω")
            return False
        
        logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–Ω–æ—Ä–∞: {donor_name}")
        
        # –°–∫–∞—á–∏–≤–∞–µ–º –ø–ª–µ–π–ª–∏—Å—Ç
        content = self.download_playlist(donor_config['url'])
        if not content:
            return False
        
        # –ü–∞—Ä—Å–∏–º –∫–∞–Ω–∞–ª—ã
        channels = self.parse_m3u_playlist(content)
        self.stats['total_parsed'] += len(channels)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –∫–∞–Ω–∞–ª
        for channel in channels:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            category = self.categorize_channel(channel)
            
            if category:
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–ª–∏ –æ–±–Ω–æ–≤–ª—è–µ–º –∫–∞–Ω–∞–ª
                self.add_or_update_channel(channel, category)
            else:
                logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å: {channel['name']} ({channel['group']})")
                self.stats['skipped_channels'] += 1
        
        return True
    
    def process_all_donors(self):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ—Ö –¥–æ–Ω–æ—Ä–æ–≤ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        logger.info("–ù–∞—á–∏–Ω–∞–µ–º —É–º–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –≤—Å–µ—Ö –¥–æ–Ω–æ—Ä–æ–≤...")
        
        donors = self.config.get('donors', {})
        if not donors:
            logger.warning("–ù–µ—Ç –¥–æ–Ω–æ—Ä–æ–≤ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
            return
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–æ–Ω–æ—Ä–æ–≤ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        sorted_donors = sorted(donors.items(), key=lambda x: x[1].get('priority', 999))
        
        for donor_name, donor_config in sorted_donors:
            try:
                self.process_donor_playlist(donor_name, donor_config)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–æ–Ω–æ—Ä–∞ {donor_name}: {e}")
        
        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.print_statistics()
    
    def print_statistics(self):
        """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        logger.info("=" * 60)
        logger.info("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –£–ú–ù–û–ô –û–ë–†–ê–ë–û–¢–ö–ò:")
        logger.info(f"–í—Å–µ–≥–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–æ –∫–∞–Ω–∞–ª–æ–≤: {self.stats['total_parsed']}")
        logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö –∫–∞–Ω–∞–ª–æ–≤: {self.stats['added_channels']}")
        logger.info(f"–û–±–Ω–æ–≤–ª–µ–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∫–∞–Ω–∞–ª–æ–≤: {self.stats['updated_channels']}")
        logger.info(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ –∫–∞–Ω–∞–ª–æ–≤: {self.stats['skipped_channels']}")
        logger.info(f"–û–±–Ω–æ–≤–ª–µ–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {len(self.stats['categories_updated'])}")
        
        if self.stats['categories_updated']:
            logger.info("–û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:")
            for category in sorted(self.stats['categories_updated']):
                logger.info(f"  - {category}")
        
        logger.info("=" * 60)

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='–£–º–Ω—ã–π –ø–∞—Ä—Å–µ—Ä IPTV –ø–ª–µ–π–ª–∏—Å—Ç–æ–≤ —Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º —Å—Å—ã–ª–æ–∫')
    parser.add_argument('--config', default='donors_config.json', help='–§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏')
    parser.add_argument('--no-update', action='store_true', help='–ù–µ –æ–±–Ω–æ–≤–ª—è—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å—Å—ã–ª–∫–∏')
    parser.add_argument('--similarity', type=float, default=0.8, help='–ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ –Ω–∞–∑–≤–∞–Ω–∏–π (0.0-1.0)')
    
    args = parser.parse_args()
    
    parser_instance = SmartPlaylistParser(args.config)
    
    if args.no_update:
        parser_instance.config['update_settings']['update_existing_urls'] = False
    
    if args.similarity:
        parser_instance.config['update_settings']['match_similarity_threshold'] = args.similarity
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ—Ö –¥–æ–Ω–æ—Ä–æ–≤
    parser_instance.process_all_donors()
    
    print("\nüéâ –£–º–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print("üìÅ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–∞–ø–∫—É 'categories/' –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –ø–ª–µ–π–ª–∏—Å—Ç–æ–≤")
    print("üíæ –ë—ç–∫–∞–ø—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'backups/categories/'")

if __name__ == "__main__":
    main()
